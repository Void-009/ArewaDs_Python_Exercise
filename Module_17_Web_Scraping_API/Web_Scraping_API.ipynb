{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Web Scraping Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1:\n",
    "Scrape the following website and store the data as json file(url = 'http://www.bu.edu/president/boston-university-facts-stats/')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page fetched successfully!\n",
      "Data successfully scraped and stored in boston_university_facts.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'http://www.bu.edu/president/boston-university-facts-stats/'\n",
    "\n",
    "# Send a GET request to fetch the HTML content of the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to retrieve data from {url}. Status code: {response.status_code}\")\n",
    "else:\n",
    "    print(\"Page fetched successfully!\")\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extracting the facts and statistics data\n",
    "    # The data seems to be organized in sections like facts and figures, and these are typically inside divs or sections\n",
    "    facts_section = soup.find_all('div', class_='fact-list-item')\n",
    "\n",
    "    # Initialize a list to store facts and stats\n",
    "    facts_data = []\n",
    "\n",
    "    for fact in facts_section:\n",
    "        # Extract the fact title (e.g., \"Founded\", \"Number of students\", etc.)\n",
    "        fact_title = fact.find('div', class_='fact-list-item-title')\n",
    "        # Extract the fact value (e.g., \"1839\", \"32,000\", etc.)\n",
    "        fact_value = fact.find('div', class_='fact-list-item-value')\n",
    "\n",
    "        if fact_title and fact_value:\n",
    "            fact_title = fact_title.get_text(strip=True)\n",
    "            fact_value = fact_value.get_text(strip=True)\n",
    "            facts_data.append({\"title\": fact_title, \"value\": fact_value})\n",
    "\n",
    "    # Store the facts data as a JSON file\n",
    "    json_file_path = 'boston_university_facts.json'\n",
    "\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(facts_data, json_file, indent=4)\n",
    "\n",
    "    print(f\"Data successfully scraped and stored in {json_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:\n",
    "Extract the table in this url (https://archive.ics.uci.edu/ml/datasets.php) and change it to a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data from https://archive.ics.uci.edu/ml/datasets.php. Status code: 404\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# URL for the UCI Machine Learning Repository dataset page\n",
    "url = 'https://archive.ics.uci.edu/ml/datasets.php'\n",
    "\n",
    "# Send a GET request to fetch the HTML content of the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to retrieve data from {url}. Status code: {response.status_code}\")\n",
    "else:\n",
    "    print(\"Page fetched successfully!\")\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the table containing the datasets\n",
    "    table = soup.find('table', {'cellspacing': '2'})\n",
    "\n",
    "    # Extract all the rows of the table (skipping the header row)\n",
    "    rows = table.find_all('tr')[1:]\n",
    "\n",
    "    # Initialize a list to store dataset information\n",
    "    datasets = []\n",
    "\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        \n",
    "        # If the row has the necessary columns, extract the information\n",
    "        if len(cols) > 1:\n",
    "            dataset_name = cols[0].get_text(strip=True)\n",
    "            dataset_link = 'https://archive.ics.uci.edu' + cols[0].find('a')['href'] if cols[0].find('a') else None\n",
    "            dataset_info = cols[1].get_text(strip=True)\n",
    "            \n",
    "            # Store the dataset information in a dictionary\n",
    "            dataset = {\n",
    "                'name': dataset_name,\n",
    "                'link': dataset_link,\n",
    "                'info': dataset_info\n",
    "            }\n",
    "            datasets.append(dataset)\n",
    "\n",
    "    # Store the datasets information in a JSON file\n",
    "    json_file_path = 'uci_datasets.json'\n",
    "\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(datasets, json_file, indent=4)\n",
    "\n",
    "    print(f\"Data successfully scraped and stored in {json_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3:\n",
    "Scrape the presidents table and store the data as json(https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States). The table is not very structured and the scrapping may take very long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page fetched successfully!\n",
      "Data successfully scraped and stored in us_presidents.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# URL of the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States\"\n",
    "\n",
    "# Send a GET request to fetch the HTML content of the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to retrieve data from {url}. Status code: {response.status_code}\")\n",
    "else:\n",
    "    print(\"Page fetched successfully!\")\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the table containing the presidents\n",
    "    # The table has the class 'wikitable', and it's the first table on the page\n",
    "    table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "    # Extract all rows of the table (excluding the header row)\n",
    "    rows = table.find_all('tr')[1:]\n",
    "\n",
    "    # Initialize a list to store presidents' data\n",
    "    presidents_data = []\n",
    "\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        \n",
    "        # If there are enough columns, we proceed to extract the data\n",
    "        if len(cols) > 3:\n",
    "            try:\n",
    "                # Extracting the president's name (column 1)\n",
    "                name = cols[1].get_text(strip=True)\n",
    "                \n",
    "                # Extracting the term in office (column 2)\n",
    "                term = cols[2].get_text(strip=True)\n",
    "                \n",
    "                # Extracting the political party (column 3)\n",
    "                party = cols[3].get_text(strip=True)\n",
    "                \n",
    "                # Storing the data as a dictionary\n",
    "                president = {\n",
    "                    'name': name,\n",
    "                    'term': term,\n",
    "                    'party': party\n",
    "                }\n",
    "                \n",
    "                # Append the data to the list\n",
    "                presidents_data.append(president)\n",
    "            except IndexError:\n",
    "                # Handle cases where rows may not have enough columns\n",
    "                print(\"Skipping row due to missing data:\", row)\n",
    "\n",
    "    # Save the data as a JSON file\n",
    "    json_file_path = 'us_presidents.json'\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(presidents_data, json_file, indent=4)\n",
    "\n",
    "    print(f\"Data successfully scraped and stored in {json_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
